{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nimport random\nimport warnings\nimport time\nimport datetime\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"id":"-iQ4gUDP6r8E","outputId":"caff2324-61f9-4d53-8eab-f999059b0259","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-10-27T10:03:51.241185Z","iopub.execute_input":"2022-10-27T10:03:51.241513Z","iopub.status.idle":"2022-10-27T10:03:51.255271Z","shell.execute_reply.started":"2022-10-27T10:03:51.241480Z","shell.execute_reply":"2022-10-27T10:03:51.254584Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# If there's a GPU available...\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","metadata":{"id":"RyRYMKRQ6r-s","outputId":"ff9b2087-0848-42b5-ebe7-17fc0e729783","execution":{"iopub.status.busy":"2022-10-27T10:03:54.124892Z","iopub.execute_input":"2022-10-27T10:03:54.125635Z","iopub.status.idle":"2022-10-27T10:03:54.208086Z","shell.execute_reply.started":"2022-10-27T10:03:54.125589Z","shell.execute_reply":"2022-10-27T10:03:54.206997Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nWe will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loading the data for modelling.\n\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint(f'Number of training tweets: {train.shape[0]}\\n')\nprint(f'Number of training tweets: {test.shape[0]}\\n')\n\ndisplay(train.sample(10))","metadata":{"id":"HDH7qw3O6r-v","outputId":"c35c82bd-0857-4962-9da9-d1e9bd32ddb1","execution":{"iopub.status.busy":"2022-10-27T10:03:58.949298Z","iopub.execute_input":"2022-10-27T10:03:58.949620Z","iopub.status.idle":"2022-10-27T10:03:59.030387Z","shell.execute_reply.started":"2022-10-27T10:03:58.949589Z","shell.execute_reply":"2022-10-27T10:03:59.029482Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of training tweets: 7613\n\nNumber of training tweets: 3263\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        id      keyword               location  \\\n2644  3796  destruction                    NaN   \n2227  3185       deluge                    NaN   \n5448  7769       police                     UK   \n132    191   aftershock                    NaN   \n6845  9810       trauma  Montgomery County, MD   \n5559  7934    rainstorm                    NaN   \n1765  2538    collision                    NaN   \n1817  2611      crashed                    NaN   \n6810  9756      tragedy        Los Angeles, CA   \n4398  6254    hijacking          Athens,Greece   \n\n                                                   text  target  \n2644  So you have a new weapon that can cause un-ima...       1  \n2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n132   Aftershock back to school kick off was great. ...       0  \n6845  in response to trauma Children of Addicts deve...       0  \n5559  @Calum5SOS you look like you got caught in a r...       0  \n1765  my favorite lady came to our volunteer meeting...       1  \n1817  @brianroemmele UX fail of EMV - people want to...       1  \n6810  Can't find my ariana grande shirt  this is a f...       0  \n4398  The Murderous Story Of AmericaÛªs First Hijac...       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2644</th>\n      <td>3796</td>\n      <td>destruction</td>\n      <td>NaN</td>\n      <td>So you have a new weapon that can cause un-ima...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2227</th>\n      <td>3185</td>\n      <td>deluge</td>\n      <td>NaN</td>\n      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5448</th>\n      <td>7769</td>\n      <td>police</td>\n      <td>UK</td>\n      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>191</td>\n      <td>aftershock</td>\n      <td>NaN</td>\n      <td>Aftershock back to school kick off was great. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6845</th>\n      <td>9810</td>\n      <td>trauma</td>\n      <td>Montgomery County, MD</td>\n      <td>in response to trauma Children of Addicts deve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5559</th>\n      <td>7934</td>\n      <td>rainstorm</td>\n      <td>NaN</td>\n      <td>@Calum5SOS you look like you got caught in a r...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>2538</td>\n      <td>collision</td>\n      <td>NaN</td>\n      <td>my favorite lady came to our volunteer meeting...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1817</th>\n      <td>2611</td>\n      <td>crashed</td>\n      <td>NaN</td>\n      <td>@brianroemmele UX fail of EMV - people want to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6810</th>\n      <td>9756</td>\n      <td>tragedy</td>\n      <td>Los Angeles, CA</td>\n      <td>Can't find my ariana grande shirt  this is a f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4398</th>\n      <td>6254</td>\n      <td>hijacking</td>\n      <td>Athens,Greece</td>\n      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Setting target variables, creating combined data and saving index for dividing combined data later.\n\nlabels = train['target'].values\nidx = len(labels)\ncombined = pd.concat([train, test])\ncombined = combined.text.values","metadata":{"id":"CmobtZpn6r-y","execution":{"iopub.status.busy":"2022-10-27T10:04:04.860321Z","iopub.execute_input":"2022-10-27T10:04:04.860643Z","iopub.status.idle":"2022-10-27T10:04:04.872848Z","shell.execute_reply.started":"2022-10-27T10:04:04.860613Z","shell.execute_reply":"2022-10-27T10:04:04.872061Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Tokenizing the combined text data using bert tokenizer.\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","metadata":{"id":"GFP3p2aA6r-1","execution":{"iopub.status.busy":"2022-10-27T10:04:08.280039Z","iopub.execute_input":"2022-10-27T10:04:08.280436Z","iopub.status.idle":"2022-10-27T10:04:08.691004Z","shell.execute_reply.started":"2022-10-27T10:04:08.280403Z","shell.execute_reply":"2022-10-27T10:04:08.689982Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8034e623f31840729a0c4b4f470beb6e"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print the original tweet.\n\nprint(' Original: ', combined[0])\n\n# Print the tweet split into tokens.\n\nprint('Tokenized: ', tokenizer.tokenize(combined[0]))\n\n# Print the sentence mapped to token ID's.\n\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))","metadata":{"id":"y0_JPi_r6r-3","outputId":"0aa56d6d-dab8-4de4-98dd-073548e3ffd5","execution":{"iopub.status.busy":"2022-10-27T10:04:12.956987Z","iopub.execute_input":"2022-10-27T10:04:12.957505Z","iopub.status.idle":"2022-10-27T10:04:12.966999Z","shell.execute_reply.started":"2022-10-27T10:04:12.957469Z","shell.execute_reply":"2022-10-27T10:04:12.966034Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":" Original:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\nTokenized:  ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', '#', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\nToken IDs:  [2256, 15616, 2024, 1996, 3114, 1997, 2023, 1001, 8372, 2089, 16455, 9641, 2149, 2035]\n","output_type":"stream"}]},{"cell_type":"code","source":"max_len = 0\n\n# For every sentence...\n\nfor text in combined:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    \n    input_ids = tokenizer.encode(text, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    \n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","metadata":{"id":"Ap4apYT16r-4","outputId":"ff808191-9a93-458f-e82f-a6f24eef749f","execution":{"iopub.status.busy":"2022-10-27T10:04:16.652528Z","iopub.execute_input":"2022-10-27T10:04:16.652846Z","iopub.status.idle":"2022-10-27T10:04:22.566814Z","shell.execute_reply.started":"2022-10-27T10:04:16.652816Z","shell.execute_reply":"2022-10-27T10:04:22.565823Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Max sentence length:  84\n","output_type":"stream"}]},{"cell_type":"code","source":"# Making list of sentence lenghts:\n\ntoken_lens = []\n\nfor text in combined:\n    tokens = tokenizer.encode(text, max_length = 512)\n    token_lens.append(len(tokens))","metadata":{"execution":{"iopub.status.busy":"2022-10-27T10:04:27.208624Z","iopub.execute_input":"2022-10-27T10:04:27.208936Z","iopub.status.idle":"2022-10-27T10:04:33.113953Z","shell.execute_reply.started":"2022-10-27T10:04:27.208907Z","shell.execute_reply":"2022-10-27T10:04:33.113189Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Splitting the train test data after tokenizing.\n\ntrain= combined[:idx]\ntest = combined[idx:]\ntrain.shape","metadata":{"id":"V5nQqKaO6r-6","outputId":"6bedb450-7dfb-4132-eda9-2283f0a27fb4","execution":{"iopub.status.busy":"2022-10-27T10:04:35.517027Z","iopub.execute_input":"2022-10-27T10:04:35.517390Z","iopub.status.idle":"2022-10-27T10:04:35.523814Z","shell.execute_reply.started":"2022-10-27T10:04:35.517358Z","shell.execute_reply":"2022-10-27T10:04:35.522989Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(7613,)"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_map(sentence,labs='None'):\n    \n    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n    \n    global labels\n    \n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    \n    for text in sentence:\n        #   \"encode_plus\" will:\n        \n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        \n        encoded_dict = tokenizer.encode_plus(\n                            text,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation='longest_first', # Activate and control truncation\n                            max_length = 84,           # Max length according to our text data.\n                            pad_to_max_length = True, # Pad & truncate all sentences.\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the id list. \n        \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        \n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    \n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n        labels = torch.tensor(labels)\n        return input_ids, attention_masks, labels\n    else:\n        return input_ids, attention_masks","metadata":{"id":"tqbud3--6r-8","execution":{"iopub.status.busy":"2022-10-27T10:04:38.466908Z","iopub.execute_input":"2022-10-27T10:04:38.467256Z","iopub.status.idle":"2022-10-27T10:04:38.475402Z","shell.execute_reply.started":"2022-10-27T10:04:38.467225Z","shell.execute_reply":"2022-10-27T10:04:38.474543Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Tokenizing all of the train test sentences and mapping the tokens to their word IDs.\n\ninput_ids, attention_masks, labels = tokenize_map(train, labels)\ntest_input_ids, test_attention_masks= tokenize_map(test)","metadata":{"id":"zzUBnO5y6r--","execution":{"iopub.status.busy":"2022-10-27T10:04:45.891773Z","iopub.execute_input":"2022-10-27T10:04:45.892088Z","iopub.status.idle":"2022-10-27T10:04:52.446575Z","shell.execute_reply.started":"2022-10-27T10:04:45.892057Z","shell.execute_reply":"2022-10-27T10:04:52.445650Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 80-20 train-validation split.\n\n# Calculate the number of samples to include in each set.\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"id":"AF-cP9956r-_","outputId":"5a1e09ff-b85c-433a-a380-2146c38f30fb","execution":{"iopub.status.busy":"2022-10-27T10:04:57.899116Z","iopub.execute_input":"2022-10-27T10:04:57.899429Z","iopub.status.idle":"2022-10-27T10:04:57.913160Z","shell.execute_reply.started":"2022-10-27T10:04:57.899400Z","shell.execute_reply":"2022-10-27T10:04:57.912298Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"6,090 training samples\n1,523 validation samples\n","output_type":"stream"}]},{"cell_type":"code","source":"# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n\nbatch_size = 32\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \n\ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = batch_size # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\n\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = batch_size # Evaluate with this batch size.\n        )","metadata":{"id":"dZL9--546r_C","execution":{"iopub.status.busy":"2022-10-27T10:05:02.533343Z","iopub.execute_input":"2022-10-27T10:05:02.533709Z","iopub.status.idle":"2022-10-27T10:05:02.539921Z","shell.execute_reply.started":"2022-10-27T10:05:02.533677Z","shell.execute_reply":"2022-10-27T10:05:02.539083Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"prediction_data = TensorDataset(test_input_ids, test_attention_masks)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)","metadata":{"id":"rogeV1Nj6r_E","execution":{"iopub.status.busy":"2022-10-27T10:05:05.980972Z","iopub.execute_input":"2022-10-27T10:05:05.981312Z","iopub.status.idle":"2022-10-27T10:05:05.985951Z","shell.execute_reply.started":"2022-10-27T10:05:05.981278Z","shell.execute_reply":"2022-10-27T10:05:05.984950Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the device which we set GPU in our case.\n\nmodel.to(device)","metadata":{"id":"BityYQ8J6r_G","outputId":"04838d37-af3e-4b47-82ef-be6e6a0a6719","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-10-27T10:05:14.617778Z","iopub.execute_input":"2022-10-27T10:05:14.618158Z","iopub.status.idle":"2022-10-27T10:06:14.799220Z","shell.execute_reply.started":"2022-10-27T10:05:14.618116Z","shell.execute_reply":"2022-10-27T10:06:14.798398Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=434.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605f2335776c492c966539e540539925"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba964f4e0d34a818e3dff9e109a7cae"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n      (position_embeddings): Embedding(512, 1024)\n      (token_type_embeddings): Embedding(2, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (12): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (13): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (14): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (15): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (16): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (17): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (18): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (19): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (20): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (21): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (22): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (23): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Get all of the model's parameters as a list of tuples:\n\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))","metadata":{"id":"qI49270K6r_I","outputId":"d90f98b0-f706-423f-8189-c9e625edfb61","execution":{"iopub.status.busy":"2022-10-27T10:06:14.816970Z","iopub.execute_input":"2022-10-27T10:06:14.817294Z","iopub.status.idle":"2022-10-27T10:06:14.965517Z","shell.execute_reply.started":"2022-10-27T10:06:14.817269Z","shell.execute_reply":"2022-10-27T10:06:14.964129Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"The BERT model has 393 different named parameters.\n\n==== Embedding Layer ====\n\nbert.embeddings.word_embeddings.weight                  (30522, 1024)\nbert.embeddings.position_embeddings.weight               (512, 1024)\nbert.embeddings.token_type_embeddings.weight               (2, 1024)\nbert.embeddings.LayerNorm.weight                             (1024,)\nbert.embeddings.LayerNorm.bias                               (1024,)\n\n==== First Transformer ====\n\nbert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\nbert.encoder.layer.0.attention.self.query.bias               (1024,)\nbert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\nbert.encoder.layer.0.attention.self.key.bias                 (1024,)\nbert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\nbert.encoder.layer.0.attention.self.value.bias               (1024,)\nbert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\nbert.encoder.layer.0.attention.output.dense.bias             (1024,)\nbert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\nbert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\nbert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\nbert.encoder.layer.0.intermediate.dense.bias                 (4096,)\nbert.encoder.layer.0.output.dense.weight                (1024, 4096)\nbert.encoder.layer.0.output.dense.bias                       (1024,)\nbert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\nbert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n\n==== Output Layer ====\n\nbert.pooler.dense.weight                                (1024, 1024)\nbert.pooler.dense.bias                                       (1024,)\nclassifier.weight                                          (2, 1024)\nclassifier.bias                                                 (2,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n\n# The 'W' stands for 'Weight Decay fix' probably...\n\noptimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate\n                  eps = 1e-8 # args.adam_epsilon\n                )","metadata":{"id":"A7Qy4SHS6r_K","execution":{"iopub.status.busy":"2022-10-27T10:06:27.020980Z","iopub.execute_input":"2022-10-27T10:06:27.021313Z","iopub.status.idle":"2022-10-27T10:06:27.027713Z","shell.execute_reply.started":"2022-10-27T10:06:27.021280Z","shell.execute_reply":"2022-10-27T10:06:27.026673Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n\n# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n\nepochs = 2\n\n# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"id":"V2_EBwu96r_N","execution":{"iopub.status.busy":"2022-10-27T10:06:33.122752Z","iopub.execute_input":"2022-10-27T10:06:33.123166Z","iopub.status.idle":"2022-10-27T10:06:33.128151Z","shell.execute_reply.started":"2022-10-27T10:06:33.123087Z","shell.execute_reply":"2022-10-27T10:06:33.127292Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    \n    \"\"\"A function for calculating accuracy scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return accuracy_score(labels_flat, pred_flat)\n\ndef flat_f1(preds, labels):\n    \n    \"\"\"A function for calculating f1 scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return f1_score(labels_flat, pred_flat)","metadata":{"id":"yBO_0cQf6r_P","execution":{"iopub.status.busy":"2022-10-27T10:06:37.291220Z","iopub.execute_input":"2022-10-27T10:06:37.291552Z","iopub.status.idle":"2022-10-27T10:06:37.297849Z","shell.execute_reply.started":"2022-10-27T10:06:37.291519Z","shell.execute_reply":"2022-10-27T10:06:37.296706Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):    \n    \n    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"id":"ZZc7gqT76r_R","execution":{"iopub.status.busy":"2022-10-27T10:06:40.172216Z","iopub.execute_input":"2022-10-27T10:06:40.172541Z","iopub.status.idle":"2022-10-27T10:06:40.177494Z","shell.execute_reply.started":"2022-10-27T10:06:40.172511Z","shell.execute_reply":"2022-10-27T10:06:40.176455Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# This training code is based on the `run_glue.py` script here:\n\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n\n# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n\ntraining_stats = []\n\n# Measure the total training time for the whole run.\n\ntotal_t0 = time.time()\n\n# For each epoch...\n\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print('')\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes:\n    \n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    \n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n    \n    # `dropout` and `batchnorm` layers behave differently during training vs. test ,\n    # source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n    \n    model.train()\n\n    # For each batch of training data...\n    \n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the device(gpu in our case) using the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        \n        b_input_ids = batch[0].to(device).to(torch.int64)\n        b_input_mask = batch[1].to(device).to(torch.int64)\n        b_labels = batch[2].to(device).to(torch.int64)\n\n        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n        # Source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n        \n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is down here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n        \n        # It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\n        # And the 'logits' (the model outputs prior to activation.)\n        \n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \n        # `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n        \n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        \n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0 This is to help prevent the 'exploding gradients' problem.\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        \n        # The optimizer dictates the 'update rule'(How the parameters are modified based on their gradients, the learning rate, etc.)\n        \n        optimizer.step()\n\n        # Update the learning rate.\n        \n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    \n    training_time = format_time(time.time() - t0)\n\n    print('')\n    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n    print('  Training epcoh took: {:}'.format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on our validation set.\n\n    print('')\n    print('Running Validation...')\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n    \n    model.eval()\n\n    # Tracking variables:\n    \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    total_eval_f1 = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch.\n    \n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        \n        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n        \n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training part).\n        \n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is down here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n            # Get the 'logits' output by the model. The 'logits' are the output values prior to applying an activation function like the softmax.\n            \n            (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        # Accumulate the validation loss.\n        \n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU:\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n        \n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        total_eval_f1 += flat_f1(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    \n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n    \n    # Report the final f1 score for this validation run.\n    \n    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n    print('  F1: {0:.2f}'.format(avg_val_f1))\n\n    # Calculate the average loss over all of the batches.\n    \n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    \n    \n    # Measure how long the validation run took:\n    \n    validation_time = format_time(time.time() - t0)\n    \n    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n    print('  Validation took: {:}'.format(validation_time))\n\n    # Record all statistics from this epoch.\n    \n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Val_F1' : avg_val_f1,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint('')\nprint('Training complete!')\n\nprint('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))","metadata":{"id":"RNBD02-66r_T","outputId":"31ddb2d9-9f0e-4deb-97df-0e9ca81a1ef3","execution":{"iopub.status.busy":"2022-10-27T10:06:45.988441Z","iopub.execute_input":"2022-10-27T10:06:45.988759Z","iopub.status.idle":"2022-10-27T10:12:34.385171Z","shell.execute_reply.started":"2022-10-27T10:06:45.988728Z","shell.execute_reply":"2022-10-27T10:12:34.384229Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\n======== Epoch 1 / 2 ========\nTraining...\n  Batch    50  of    191.    Elapsed: 0:00:43.\n  Batch   100  of    191.    Elapsed: 0:01:25.\n  Batch   150  of    191.    Elapsed: 0:02:08.\n\n  Average training loss: 0.51\n  Training epcoh took: 0:02:42\n\nRunning Validation...\n  Accuracy: 0.80\n  F1: 0.76\n  Validation Loss: 0.45\n  Validation took: 0:00:13\n\n======== Epoch 2 / 2 ========\nTraining...\n  Batch    50  of    191.    Elapsed: 0:00:42.\n  Batch   100  of    191.    Elapsed: 0:01:25.\n  Batch   150  of    191.    Elapsed: 0:02:07.\n\n  Average training loss: 0.39\n  Training epcoh took: 0:02:41\n\nRunning Validation...\n  Accuracy: 0.82\n  F1: 0.77\n  Validation Loss: 0.42\n  Validation took: 0:00:13\n\nTraining complete!\nTotal training took 0:05:48 (h:mm:ss)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display floats with two decimal places.\n\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\n\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\n\ndf_stats = df_stats.set_index('epoch')\n\n# Display the table.\n\ndisplay(df_stats)","metadata":{"id":"UU10XeVz6r_V","outputId":"85620a0b-c516-401b-e699-f4979b5440ac","execution":{"iopub.status.busy":"2022-10-27T10:12:50.728416Z","iopub.execute_input":"2022-10-27T10:12:50.728728Z","iopub.status.idle":"2022-10-27T10:12:50.747043Z","shell.execute_reply.started":"2022-10-27T10:12:50.728698Z","shell.execute_reply":"2022-10-27T10:12:50.746166Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"       Training Loss  Valid. Loss  Valid. Accur.  Val_F1 Training Time  \\\nepoch                                                                    \n1               0.51         0.45           0.80    0.76       0:02:42   \n2               0.39         0.42           0.82    0.77       0:02:41   \n\n      Validation Time  \nepoch                  \n1             0:00:13  \n2             0:00:13  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Accur.</th>\n      <th>Val_F1</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.51</td>\n      <td>0.45</td>\n      <td>0.80</td>\n      <td>0.76</td>\n      <td>0:02:42</td>\n      <td>0:00:13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.39</td>\n      <td>0.42</td>\n      <td>0.82</td>\n      <td>0.77</td>\n      <td>0:02:41</td>\n      <td>0:00:13</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Increase the plot size and font size:\n\nfig, axes = plt.subplots(figsize=(12,8))\n\n# Plot the learning curve:\n\nplt.plot(df_stats['Training Loss'], 'b-o', label='Training')\nplt.plot(df_stats['Valid. Loss'], 'g-o', label='Validation')\n\n# Label the plot:\n\nplt.title('Training & Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\nplt.legend()\nplt.xticks([1, 2])\n\nplt.show()","metadata":{"id":"vybyY0tk6r_Y","outputId":"29278bd4-e8e1-4253-8808-d756056658b5","execution":{"iopub.status.busy":"2022-10-27T10:13:37.529036Z","iopub.execute_input":"2022-10-27T10:13:37.529386Z","iopub.status.idle":"2022-10-27T10:13:37.689895Z","shell.execute_reply.started":"2022-10-27T10:13:37.529347Z","shell.execute_reply":"2022-10-27T10:13:37.689058Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x576 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Prediction on test set:\n\nprint('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n\n# Put model in evaluation mode:\n\nmodel.eval()\n\n# Tracking variables :\n\npredictions = []\n\n# Predict:\n\nfor batch in prediction_dataloader:\n    \n  # Add batch to GPU\n\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader:\n    \n  b_input_ids, b_input_mask, = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction:\n\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions:\n    \n      outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n  logits = outputs[0]\n\n  # Move logits and labels to CPU:\n    \n  logits = logits.detach().cpu().numpy()\n \n  \n  # Store predictions and true labels:\n    \n  predictions.append(logits)\n\n\nprint('    DONE.')","metadata":{"id":"ZIkWerb86r_Z","outputId":"7ab78938-a1bb-4daa-a9f9-93ef0a883d59","execution":{"iopub.status.busy":"2022-10-27T10:13:56.238789Z","iopub.execute_input":"2022-10-27T10:13:56.239136Z","iopub.status.idle":"2022-10-27T10:14:23.138208Z","shell.execute_reply.started":"2022-10-27T10:13:56.239080Z","shell.execute_reply":"2022-10-27T10:14:23.137276Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Predicting labels for 3,263 test sentences...\n    DONE.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting list of predictions and then choosing the target value with using argmax on probabilities.\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","metadata":{"id":"WBHLkwr06r_f","execution":{"iopub.status.busy":"2022-10-27T10:14:30.511395Z","iopub.execute_input":"2022-10-27T10:14:30.511712Z","iopub.status.idle":"2022-10-27T10:14:30.519701Z","shell.execute_reply.started":"2022-10-27T10:14:30.511681Z","shell.execute_reply":"2022-10-27T10:14:30.518797Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Creating submission data.\n\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['target'] = flat_predictions\nsubmission.head(10)","metadata":{"id":"-DVmThnd6r_h","outputId":"f7163ae0-e44d-4539-a302-1445a1d215c5","execution":{"iopub.status.busy":"2022-10-27T10:14:34.976096Z","iopub.execute_input":"2022-10-27T10:14:34.976584Z","iopub.status.idle":"2022-10-27T10:14:34.998820Z","shell.execute_reply.started":"2022-10-27T10:14:34.976545Z","shell.execute_reply":"2022-10-27T10:14:34.998059Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"   id  target\n0   0       1\n1   2       1\n2   3       1\n3   9       1\n4  11       1\n5  12       0\n6  21       0\n7  22       0\n8  27       0\n9  29       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Saving submission to '.csv' file:\n\nsubmission.to_csv('submission.csv', index=False, header=True)","metadata":{"id":"CXxu-Qcp6r_i","execution":{"iopub.status.busy":"2022-10-27T10:14:41.902942Z","iopub.execute_input":"2022-10-27T10:14:41.903294Z","iopub.status.idle":"2022-10-27T10:14:42.079541Z","shell.execute_reply.started":"2022-10-27T10:14:41.903262Z","shell.execute_reply":"2022-10-27T10:14:42.078694Z"},"trusted":true},"execution_count":31,"outputs":[]}]}